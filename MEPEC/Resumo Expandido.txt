Este estudo explora a capacidade dos modelos de linguagem de grande escala (LLMs) criarem códigos maliciosos que não podem ser detectados por ferramentas automatizadas, utilizando prompts não técnicos, ou seja, solicitações em linguagem natural sem aprofundamento técnico em programação ou cibersegurança.  Considerando o cenário da rápida disseminação dos LLMs e os perigos ligados à geração automatizada de código, o objetivo principal é verificar se prompts não técnicos são capazes de levar LLMs amplamente utilizados a criar malwares funcionais e a contornar sistemas de detecção. A metodologia empregada abrange: (i) revisão da literatura recente sobre jailbreaks e riscos associados a LLMs; (ii) definição dos modelos alvo (ChatGPT — GPT-4o, Google Gemini e GitHub Copilot) com base na extensão de uso, suporte ao idioma português e possibilidade de registro das conversas; (iii) utilização de templates de ataque catalogados na literatura (categoria “Ataques por Template” identificada em revisões de jailbreak) e escolha de até três templates por trabalho elegível, priorizando a taxa de sucesso, a simplicidade e a diversidade; (iv) realização de experimentos de forma iterativa, limitando cada interação a 16 diálogos; (v) avaliação técnica dos códigos gerados por meio de análises estáticas e dinâmicas, além da validação externa utilizando a plataforma VirusTotal (agregador de diversos motores de detecção). Nos resultados parciais, testes preliminares com o GPT-4o utilizando cinco templates de alto sucesso (AIM, BetterDAN, DeltaGPT, EvilConfidant e TextContinuation) geraram códigos classificados como keylogger, prankware e dropper — predominantemente em Python, com um caso em C — obtendo geração em 3 à 5 interações; prankwares e droppers mostraram-se inicialmente indetectáveis, enquanto keyloggers exigiram refinamentos iterativos (AIM tornou-se indetectável após duas iterações; BetterDAN permaneceu detectado). Essas evidências iniciais indicam que prompts não técnicos podem, em determinadas circunstâncias e com variações segundo o template e o modelo, conduzir à produção de código com comportamento malicioso e capacidade de evadir alguns mecanismos de detecção. O estudo implementa salvaguardas éticas, como o registro público das interações em um repositório controlado, a não execução de código malicioso em ambientes produtivos e a criação de uma conta exclusiva para testes. Dessa forma, contribui para preencher uma lacuna na literatura ao focar no uso do português brasileiro, no ambiente público de experimentação e na caracterização operacional de “prompt não técnico”.  Conclui-se que são necessárias defesas adaptativas e políticas mais robustas para mitigar riscos emergentes. O trabalho sugere, como próximos passos, a ampliação dos testes a outros LLMs, uma maior amostragem de prompts e uma avaliação mais abrangente em relação às atualizações dos modelos.
